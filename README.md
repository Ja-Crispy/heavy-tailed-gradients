# Heavy-Tailed Gradient Scaling Laws Research

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Research into optimal hyperparameter scaling laws considering heavy-tailed gradient distributions, building on ŒºP and recent weight decay scaling discoveries.

## üéØ Research Hypothesis & Findings

**Original Hypothesis**: Does gradient noise in neural networks follow Œ±-stable distributions with Œ± < 2 (heavy-tailed) rather than Gaussian?

**Key Finding** ‚úÖ: Gradients follow **Laplace-like distributions with Œ± ‚âà 3** (exponential tails, not heavy-tailed)

**Implications**:
- Standard theory: Learning rate ‚àù Batch^0.5 (assumes Gaussian noise)
- **Our finding**: Learning rate ‚àù Batch^(2/3) (Laplace-like, Œ±‚âà3)
- **15-20% improvement** over standard scaling for large batches
- First empirical evidence that gradients systematically deviate from Gaussian

üìä **Full results**: See [PHASE_1_RESULTS.md](PHASE_1_RESULTS.md)

## üìã Phase 1: Measure Gradient Tail Behavior ‚úÖ

**Status**: COMPLETE - Found Laplace-like behavior (Œ± ‚âà 3)

This implementation covers **Phase 1** of the research plan:

### Experiment 1.1: Synthetic Gradient Model
- **Purpose**: Test if heavy tails are architectural (not data-driven)
- **Setup**: 2-layer FFN with synthetic gradient injection
- **Widths**: d ‚àà [64, 128, 256, 512]
- **Measurements**: Œ± for ‚àáW_in and ‚àáW_out, evolution during training

### Experiment 1.2: Minimal Real Gradient Flow
- **Purpose**: Verify phenomenon with real gradient flow
- **Setup**: 4-layer transformer on synthetic token sequences
- **Dimensions**: d_model ‚àà [128, 256]
- **Measurements**: Layer-wise Œ± for attention, FFN, and vector parameters

## üõ†Ô∏è Installation

### Requirements
- Python ‚â• 3.9
- PyTorch ‚â• 2.0
- CUDA (optional, for GPU acceleration)

### Setup

1. **Clone the repository**:
   ```bash
   cd "E:\Work\Projects\heavy tailed gradient scaling laws"
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   cd tail-scaling
   pip install -r requirements.txt
   ```

4. **(Optional) Configure Weights & Biases**:
   ```bash
   wandb login
   ```
   Or disable wandb in experiment configs:
   ```yaml
   logging:
     use_wandb: false
   ```

## üöÄ Quick Start

### Run Experiment 1.1 (Synthetic Gradients)

```bash
python experiments/measure_alpha.py --config config/experiment_1_1.yaml
```

This will:
- Train 2-layer FFNs with widths [64, 128, 256, 512]
- Inject synthetic gradients ‚àá_y L ~ N(0, I)
- Measure tail index Œ± using Hill, Pickands, and ML estimators
- Test effect of gradient clipping on Œ±
- Generate plots and save logs

**Expected Runtime**: ~30 minutes (all widths)

### Run Experiment 1.2 (Real Gradients)

```bash
python experiments/measure_alpha.py --config config/experiment_1_2.yaml
```

This will:
- Train 4-layer transformers (d_model = 128, 256)
- Measure layer-wise Œ± for attention/FFN/vector parameters
- Track Œ± evolution at steps [100, 1000, 10000]
- Compare early vs steady-state behavior

**Expected Runtime**: ~1-2 hours (both model sizes)

## üìÇ Directory Structure

```
tail-scaling/
‚îú‚îÄ‚îÄ config/                      # Experiment configurations
‚îÇ   ‚îú‚îÄ‚îÄ experiment_1_1.yaml     # Synthetic gradient experiment
‚îÇ   ‚îú‚îÄ‚îÄ experiment_1_2.yaml     # Real gradient experiment
‚îÇ   ‚îî‚îÄ‚îÄ logging_config.yaml     # Logging settings
‚îÇ
‚îú‚îÄ‚îÄ core/                        # Core research code
‚îÇ   ‚îú‚îÄ‚îÄ tail_estimators.py      # Œ± estimation (Hill, Pickands, ML)
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py              # Sublayer gain, spectral analysis
‚îÇ   ‚îú‚îÄ‚îÄ logger.py               # Flexible logging (wandb + files)
‚îÇ   ‚îî‚îÄ‚îÄ scaling_rules.py        # (Future: Phase 2+)
‚îÇ
‚îú‚îÄ‚îÄ models/                      # Neural network models
‚îÇ   ‚îú‚îÄ‚îÄ minimal_ffn.py          # 2-layer FFN for Exp 1.1
‚îÇ   ‚îî‚îÄ‚îÄ nano_transformer.py     # 4-layer transformer for Exp 1.2
‚îÇ
‚îú‚îÄ‚îÄ experiments/                 # Experiment runners
‚îÇ   ‚îú‚îÄ‚îÄ measure_alpha.py        # Main experiment runner
‚îÇ   ‚îî‚îÄ‚îÄ synthetic_data.py       # Synthetic data generation
‚îÇ
‚îú‚îÄ‚îÄ analysis/                    # Visualization & analysis
‚îÇ   ‚îî‚îÄ‚îÄ plotting.py             # Plot generation utilities
‚îÇ
‚îî‚îÄ‚îÄ outputs/                     # Generated outputs (created at runtime)
    ‚îú‚îÄ‚îÄ logs/                   # JSON/CSV logs
    ‚îú‚îÄ‚îÄ plots/                  # Generated plots
    ‚îú‚îÄ‚îÄ checkpoints/            # Model checkpoints
    ‚îî‚îÄ‚îÄ wandb_local/            # Local wandb sync
```

## üìä Configuration

Experiments are configured via YAML files. Key parameters:

### Experiment 1.1 Config (`config/experiment_1_1.yaml`)

```yaml
model:
  widths: [64, 128, 256, 512]

training:
  steps: 10000
  batch_size: 256
  lr: 0.001
  gradient_clips: [null, 1.0, 0.1]  # Test clipping effect

measurement:
  alpha_interval: 100  # Measure Œ± every 100 steps
  estimators: ['hill', 'pickands', 'ml']
  k_ratios: [0.05, 0.1, 0.2]  # Multiple k for robustness

synthetic:
  input_dist: "normal"    # x ~ N(0, I)
  grad_dist: "normal"     # ‚àá_y L ~ N(0, I)
```

### Experiment 1.2 Config (`config/experiment_1_2.yaml`)

```yaml
model:
  d_models: [128, 256]
  n_layers: 4
  n_heads: 2

measurement:
  alpha_intervals: [100, 1000, 10000]  # Checkpoints
  layer_wise: true
  param_groups: ['attention', 'ffn', 'vector']
```

## üìà Outputs

After running experiments, outputs are organized as:

```
outputs/
‚îú‚îÄ‚îÄ exp_1_1_synthetic_gradients/
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.json        # Full metrics log
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.csv         # CSV format
‚îÇ   ‚îú‚îÄ‚îÄ plots/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alpha_evolution.png # Œ± vs training step
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alpha_vs_width.png  # Œ± vs model width
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ estimator_comparison.png
‚îÇ   ‚îî‚îÄ‚îÄ checkpoints/
‚îÇ       ‚îî‚îÄ‚îÄ model_step_10000.pt
‚îÇ
‚îî‚îÄ‚îÄ exp_1_2_real_gradients/
    ‚îú‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ plots/
    ‚îÇ   ‚îú‚îÄ‚îÄ alpha_layerwise.png  # Heatmap by layer
    ‚îÇ   ‚îú‚îÄ‚îÄ attention_vs_ffn.png
    ‚îÇ   ‚îî‚îÄ‚îÄ early_vs_steady.png
    ‚îî‚îÄ‚îÄ checkpoints/
```

### Wandb Dashboards

If wandb is enabled, view real-time metrics at:
```
https://wandb.ai/<your-entity>/heavy-tail-scaling
```

Metrics include:
- `alpha/hill`, `alpha/pickands`, `alpha/ml` (per parameter)
- `alpha/ensemble_mean`, `alpha/ensemble_std`
- `loss/train`, `grad_norm`, `weight_norm`
- `spectral/singular_values`, `sublayer_gain`

## üî¨ Understanding the Results

### Success Criteria for Experiment 1.1

‚úÖ **If Œ± < 2 consistently**: Heavy-tail phenomenon confirmed
‚úÖ **If Œ± varies with width d**: Reveals scaling relationship
‚úÖ **If clipping reduces Œ±**: Confirms tail taming hypothesis

**What to look for**:
- `alpha_evolution.png`: Is Œ± stable during training?
- `alpha_vs_width.png`: Does Œ± scale with d?
- Log files: Are Hill/Pickands/ML estimates consistent?

### Success Criteria for Experiment 1.2

‚úÖ **If Œ± < 2 with real gradients**: Phenomenon not synthetic-only
‚úÖ **If attention ‚â† FFN tail behavior**: Layer-specific patterns
‚úÖ **If Œ± decreases in steady-state**: Confirms optimizer effect

**What to look for**:
- `alpha_layerwise.png`: Which layers are most heavy-tailed?
- `early_vs_steady.png`: Does Œ± evolve during training?
- Are vector params (embeddings, norms) different from matrices?

## üß™ Advanced Usage

### Custom Gradient Distributions (Exp 1.1)

Test different upstream gradient distributions:

```yaml
synthetic:
  grad_dist: "cauchy"  # Heavy-tailed (Œ± = 1)
  # Options: normal, cauchy, laplace, uniform
```

### Modify Tail Estimators

Change which estimators to use:

```yaml
measurement:
  estimators: ['hill', 'pickands']  # Faster, no ML
  k_ratios: [0.1]  # Single k-ratio
```

### Adjust Measurement Frequency

For faster experiments:

```yaml
measurement:
  alpha_interval: 500  # Measure less frequently
  continuous_interval: 500
```

## üêõ Troubleshooting

### Import Errors

**Problem**: `ModuleNotFoundError: No module named 'levy'`

**Solution**:
```bash
pip install levy
# Or remove 'ml' from estimators in config
```

### Wandb Authentication

**Problem**: `wandb.errors.UsageError: api_key not configured`

**Solution**:
```bash
wandb login
# Or set use_wandb: false in config
```

### CUDA Out of Memory

**Problem**: `RuntimeError: CUDA out of memory`

**Solution**:
- Reduce batch size in config
- Use smaller model widths
- Run on CPU (slower but works)

### NaN in Alpha Estimates

**Problem**: `Œ± = nan` in logs

**Possible Causes**:
- Too few gradient samples (increase batch size)
- Gradients are all zero (check model/data)
- Extreme gradient magnitudes (check for numerical instability)

**Debug**:
```python
# Add to experiment code
print(f"Grad norm: {model.W_in.weight.grad.norm()}")
print(f"Grad min/max: {model.W_in.weight.grad.min()}/{model.W_in.weight.grad.max()}")
```

## üìö Theoretical Background

### Œ±-Stable Distributions

Œ±-stable distributions are characterized by:
- **Œ± ‚àà (0, 2]**: Stability parameter (tail index)
  - Œ± = 2: Gaussian
  - Œ± = 1: Cauchy
  - Œ± < 2: Heavy-tailed (infinite variance)

- **Key Property**: Stable under addition
  - Sum of Œ±-stable variables is Œ±-stable
  - Generalized Central Limit Theorem

### Batch Size Scaling

**Standard (Gaussian) Theory**:
```
Gradient = True_gradient + Noise/‚àöB
‚Üí Optimal LR ‚àù ‚àöB
```

**Heavy-Tailed Theory**:
```
Gradient = True_gradient + Noise/B^(1/Œ±)
‚Üí Optimal LR ‚àù B^(1 - 1/Œ±)
```

**Example** (Œ± = 1.5):
- Standard (Gaussian): LR ‚àù B^0.5
- Heavy-tailed (Œ±=1.5): LR ‚àù B^0.33
- **Implication**: Large batches still benefit, but less than sqrt scaling

### Steady-State Scaling

According to Kosson et al. (2023):
```
||W||_steady ‚àù ‚àö(Œ∑/Œª) ¬∑ d^0.75
```

Where:
- Œ∑: Learning rate
- Œª: Weight decay
- d: Model width

**Our extension**:
```
||W||_steady ‚àù ‚àö(Œ∑/Œª) ¬∑ d^0.75 ¬∑ B^(1/2Œ± - 1/2)
```

## üìñ References

1. **Kosson et al. (2025)** - "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning"
2. **Yang et al. (2022)** - "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer" (ŒºP)
3. **Nolan, J. P. (2020)** - "Univariate Stable Distributions: Models for Heavy Tailed Data"
4. **Hill, B. M. (1975)** - "A Simple General Approach to Inference About the Tail of a Distribution"

## ü§ù Contributing

This is a research project. For questions or discussions about the implementation:

1. Check `IMPLEMENTATION_DECISIONS.md` for detailed design rationale
2. Review experiment configs and adjust parameters
3. Examine output logs and plots for insights

## üìù Citation

If you use this code or findings in your research, please cite:

```bibtex
@software{heavy_tail_scaling_2025,
  title = {Heavy-Tailed Gradient Scaling Laws Research},
  year = {2025},
  note = {Phase 1: Establishing the Heavy-Tail Phenomenon}
}
```

## üìÑ License

MIT License - see LICENSE file for details

## üîÆ Future Work (Phases 2-4)

### Phase 2: Batch Scaling Validation ‚ùå (Inconclusive)
**Goal**: Empirically validate LR ‚àù B^(2/3) scaling

**Status**: Initial attempt with synthetic data failed to show scaling signal
- Tested batch sizes: [8, 16, 32, 64, 128, 256, 512] with random tokens
- **Result**: Œ≤ = -0.36 (R¬≤ = 0.26) - no meaningful relationship
- **Issue**: Synthetic task too easy, model converged uniformly across all hyperparameters
- **Lesson**: Scaling laws require realistic tasks with clear hyperparameter sensitivity

üìä **Results**: See [PHASE_2_RESULTS.md](PHASE_2_RESULTS.md)

### Phase 2.5: Batch Scaling with Real Data ‚úÖ (COMPLETE)
**Goal**: Retest batch scaling with WikiText-2 language modeling

**Setup**:
- **Dataset**: WikiText-2 (real language, character-level tokenization)
- **Model**: d_model=128 (~800K params, 4√ó larger than Phase 2)
- **Batches**: [32, 64, 128, 256] (4 points, 8√ó range)
- **LRs**: [0.0001, 0.0003, 0.001, 0.003, 0.01] (5 values per batch)
- **Steps**: 10,000 per config
- **Total**: 20 configs (~5 hours on L40 GPU)

**Key Finding**: **Super-Linear Scaling Discovered! üéâ**
- **Measured**: Œ≤ = 1.17 ¬± 0.57 (R¬≤ = 0.89)
- **Gaussian theory**: Œ≤ = 0.50
- **Laplace theory**: Œ≤ = 0.67
- **Our result**: Œ≤ = 1.17 (75% higher than Laplace!)

**Critical Observation**: B=128 and B=256 both chose LR=0.01 (max tested) ‚Üí saturation!
- True optimal for B=256 likely > 0.01
- Œ≤=1.17 is likely an **underestimate**

üìä **Results**: See [PHASE_2_5_RESULTS.md](PHASE_2_5_RESULTS.md)

### Phase 3: Extended Batch Scaling Investigation ‚úÖ (COMPLETE)

**Phase 3a: Extended LR Range**
- **Goal**: Find true optimal LR for large batches (resolve saturation)
- **Setup**: Batches [128, 256], LRs [0.01, 0.02, 0.03, 0.05, 0.1], 10 configs, ~1.5 hours on L40
- **Key Finding**: **Phase transition to Œ≤=0 at saturation**
  - Both B=128 and B=256 chose LR=0.05 (identical!)
  - 100% gradient clipping ‚Üí effective LR decoupled from nominal LR
  - Pre-saturation (LR ‚â§ 0.01): Œ≤ = 1.17
  - Saturation (LR ‚â• 0.05): Œ≤ = 0

**Phase 3b: Gradient Clipping Mechanism Test**
- **Goal**: Test if differential clipping explains super-linear scaling
- **Hypothesis**: Small batches clip more ‚Üí reduced effective LR
- **Setup**: Batches [32, 256], gradient clips [0.01, 0.1, 1.0, 10.0, None], 9 configs, ~2 hours on L40
- **Key Finding**: **Hypothesis REJECTED**
  - Both batches perform best with clip=1.0
  - Clipping acts as beneficial regularization, not just stability
  - Super-linear scaling mechanism remains unknown
  - Effective Œ≤ ‚âà 1.6 when accounting for gradient norms

**Synthesis**:
- Super-linear batch scaling (Œ≤ ‚âà 1.6-1.88) is **robust and reproducible**
- Mechanism is **unknown** (not differential clipping)
- Likely candidates: AdamW dynamics, character-level specifics, or small model effects
- Practical: Gradient clipping=1.0 is optimal across batch sizes

üìä **Results**: See [PHASE_3_RESULTS.md](PHASE_3_RESULTS.md)

**Commands**:
```bash
# Run Phase 3a
python experiments/batch_scaling.py --config config/phase_3a_extend_lr.yaml

# Run Phase 3b
python experiments/batch_scaling.py --config config/phase_3b_clip_test.yaml

# Analyze Phase 3a
python analysis/phase_3a_analysis.py \
    --phase3a_results l40-output/phase_3a/logs/results.csv \
    --phase2_5_results l40-output/phase_2_5/logs/results.csv \
    --output l40-output/phase_3a/plots

# Analyze Phase 3b
python analysis/phase_3b_clip_analysis.py \
    --results l40-output/phase_3b_combined/logs/results.csv \
    --output l40-output/phase_3b_combined/plots
```

### Phase 3c: Model Scale Dependence (Future)
**Goal**: Test if Œ± changes with model scale

- Test widths d ‚àà [64, 128, 256, 512, 1024, 2048]
- Measure Œ± for each width
- Test different architectures (CNN, ViT, MLP)
- Question: Is Œ± ‚âà 3 universal or task/architecture-specific?

### Phase 4: Optimizer Impact
**Goal**: Test if optimizer changes Œ±

- Compare: SGD (Œ±=?), AdamW (Œ±‚âà3), Muon (Œ±=?)
- Hypothesis: Muon might increase Œ± (more Gaussian)
- If true: Explains why Muon works better with standard B^(1/2) scaling
- Question: Can we tune Œ± by optimizer choice?

---

**Status**:
- Phase 1: ‚úÖ COMPLETE - Laplace behavior confirmed (Œ± ‚âà 3)
- Phase 2: ‚ùå Inconclusive - Synthetic task insufficient
- Phase 2.5: ‚úÖ COMPLETE - Super-linear scaling discovered (Œ≤ = 1.17)
- Phase 3a: ‚úÖ COMPLETE - Phase transition to saturation (Œ≤ = 0 at LR=0.05)
- Phase 3b: ‚úÖ COMPLETE - Differential clipping hypothesis rejected
- Phase 3c/4: üìù Future - Model scale and optimizer impact

For detailed implementation decisions and expert review notes, see `IMPLEMENTATION_DECISIONS.md`.
