# Heavy-Tailed Gradient Scaling Laws Research

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Research into optimal hyperparameter scaling laws considering heavy-tailed gradient distributions, building on Î¼P and recent weight decay scaling discoveries.

## ðŸŽ¯ Research Hypothesis

**Central Question**: Does gradient noise in neural networks follow Î±-stable distributions with Î± < 2 (heavy-tailed) rather than Gaussian, and if so, how does this change optimal hyperparameter scaling laws?

**Key Implications**:
- Standard theory: Learning rate âˆ Batch^(-0.5) (assumes Gaussian noise)
- Heavy-tail theory: Learning rate âˆ Batch^(1/Î± - 1) (if Î± < 2)
- For Î± = 1.5: LR âˆ Batch^(-0.33) (weaker than sqrt scaling!)

## ðŸ“‹ Phase 1: Establish Heavy-Tail Phenomenon

This implementation covers **Phase 1** of the research plan:

### Experiment 1.1: Synthetic Gradient Model
- **Purpose**: Test if heavy tails are architectural (not data-driven)
- **Setup**: 2-layer FFN with synthetic gradient injection
- **Widths**: d âˆˆ [64, 128, 256, 512]
- **Measurements**: Î± for âˆ‡W_in and âˆ‡W_out, evolution during training

### Experiment 1.2: Minimal Real Gradient Flow
- **Purpose**: Verify phenomenon with real gradient flow
- **Setup**: 4-layer transformer on synthetic token sequences
- **Dimensions**: d_model âˆˆ [128, 256]
- **Measurements**: Layer-wise Î± for attention, FFN, and vector parameters

## ðŸ› ï¸ Installation

### Requirements
- Python â‰¥ 3.9
- PyTorch â‰¥ 2.0
- CUDA (optional, for GPU acceleration)

### Setup

1. **Clone the repository**:
   ```bash
   cd "E:\Work\Projects\heavy tailed gradient scaling laws"
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   cd tail-scaling
   pip install -r requirements.txt
   ```

4. **(Optional) Configure Weights & Biases**:
   ```bash
   wandb login
   ```
   Or disable wandb in experiment configs:
   ```yaml
   logging:
     use_wandb: false
   ```

## ðŸš€ Quick Start

### Run Experiment 1.1 (Synthetic Gradients)

```bash
python experiments/measure_alpha.py --config config/experiment_1_1.yaml
```

This will:
- Train 2-layer FFNs with widths [64, 128, 256, 512]
- Inject synthetic gradients âˆ‡_y L ~ N(0, I)
- Measure tail index Î± using Hill, Pickands, and ML estimators
- Test effect of gradient clipping on Î±
- Generate plots and save logs

**Expected Runtime**: ~30 minutes (all widths)

### Run Experiment 1.2 (Real Gradients)

```bash
python experiments/measure_alpha.py --config config/experiment_1_2.yaml
```

This will:
- Train 4-layer transformers (d_model = 128, 256)
- Measure layer-wise Î± for attention/FFN/vector parameters
- Track Î± evolution at steps [100, 1000, 10000]
- Compare early vs steady-state behavior

**Expected Runtime**: ~1-2 hours (both model sizes)

## ðŸ“‚ Directory Structure

```
tail-scaling/
â”œâ”€â”€ config/                      # Experiment configurations
â”‚   â”œâ”€â”€ experiment_1_1.yaml     # Synthetic gradient experiment
â”‚   â”œâ”€â”€ experiment_1_2.yaml     # Real gradient experiment
â”‚   â””â”€â”€ logging_config.yaml     # Logging settings
â”‚
â”œâ”€â”€ core/                        # Core research code
â”‚   â”œâ”€â”€ tail_estimators.py      # Î± estimation (Hill, Pickands, ML)
â”‚   â”œâ”€â”€ metrics.py              # Sublayer gain, spectral analysis
â”‚   â”œâ”€â”€ logger.py               # Flexible logging (wandb + files)
â”‚   â””â”€â”€ scaling_rules.py        # (Future: Phase 2+)
â”‚
â”œâ”€â”€ models/                      # Neural network models
â”‚   â”œâ”€â”€ minimal_ffn.py          # 2-layer FFN for Exp 1.1
â”‚   â””â”€â”€ nano_transformer.py     # 4-layer transformer for Exp 1.2
â”‚
â”œâ”€â”€ experiments/                 # Experiment runners
â”‚   â”œâ”€â”€ measure_alpha.py        # Main experiment runner
â”‚   â””â”€â”€ synthetic_data.py       # Synthetic data generation
â”‚
â”œâ”€â”€ analysis/                    # Visualization & analysis
â”‚   â””â”€â”€ plotting.py             # Plot generation utilities
â”‚
â””â”€â”€ outputs/                     # Generated outputs (created at runtime)
    â”œâ”€â”€ logs/                   # JSON/CSV logs
    â”œâ”€â”€ plots/                  # Generated plots
    â”œâ”€â”€ checkpoints/            # Model checkpoints
    â””â”€â”€ wandb_local/            # Local wandb sync
```

## ðŸ“Š Configuration

Experiments are configured via YAML files. Key parameters:

### Experiment 1.1 Config (`config/experiment_1_1.yaml`)

```yaml
model:
  widths: [64, 128, 256, 512]

training:
  steps: 10000
  batch_size: 256
  lr: 0.001
  gradient_clips: [null, 1.0, 0.1]  # Test clipping effect

measurement:
  alpha_interval: 100  # Measure Î± every 100 steps
  estimators: ['hill', 'pickands', 'ml']
  k_ratios: [0.05, 0.1, 0.2]  # Multiple k for robustness

synthetic:
  input_dist: "normal"    # x ~ N(0, I)
  grad_dist: "normal"     # âˆ‡_y L ~ N(0, I)
```

### Experiment 1.2 Config (`config/experiment_1_2.yaml`)

```yaml
model:
  d_models: [128, 256]
  n_layers: 4
  n_heads: 2

measurement:
  alpha_intervals: [100, 1000, 10000]  # Checkpoints
  layer_wise: true
  param_groups: ['attention', 'ffn', 'vector']
```

## ðŸ“ˆ Outputs

After running experiments, outputs are organized as:

```
outputs/
â”œâ”€â”€ exp_1_1_synthetic_gradients/
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ metrics.json        # Full metrics log
â”‚   â”‚   â””â”€â”€ metrics.csv         # CSV format
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â”œâ”€â”€ alpha_evolution.png # Î± vs training step
â”‚   â”‚   â”œâ”€â”€ alpha_vs_width.png  # Î± vs model width
â”‚   â”‚   â””â”€â”€ estimator_comparison.png
â”‚   â””â”€â”€ checkpoints/
â”‚       â””â”€â”€ model_step_10000.pt
â”‚
â””â”€â”€ exp_1_2_real_gradients/
    â”œâ”€â”€ logs/
    â”œâ”€â”€ plots/
    â”‚   â”œâ”€â”€ alpha_layerwise.png  # Heatmap by layer
    â”‚   â”œâ”€â”€ attention_vs_ffn.png
    â”‚   â””â”€â”€ early_vs_steady.png
    â””â”€â”€ checkpoints/
```

### Wandb Dashboards

If wandb is enabled, view real-time metrics at:
```
https://wandb.ai/<your-entity>/heavy-tail-scaling
```

Metrics include:
- `alpha/hill`, `alpha/pickands`, `alpha/ml` (per parameter)
- `alpha/ensemble_mean`, `alpha/ensemble_std`
- `loss/train`, `grad_norm`, `weight_norm`
- `spectral/singular_values`, `sublayer_gain`

## ðŸ”¬ Understanding the Results

### Success Criteria for Experiment 1.1

âœ… **If Î± < 2 consistently**: Heavy-tail phenomenon confirmed
âœ… **If Î± varies with width d**: Reveals scaling relationship
âœ… **If clipping reduces Î±**: Confirms tail taming hypothesis

**What to look for**:
- `alpha_evolution.png`: Is Î± stable during training?
- `alpha_vs_width.png`: Does Î± scale with d?
- Log files: Are Hill/Pickands/ML estimates consistent?

### Success Criteria for Experiment 1.2

âœ… **If Î± < 2 with real gradients**: Phenomenon not synthetic-only
âœ… **If attention â‰  FFN tail behavior**: Layer-specific patterns
âœ… **If Î± decreases in steady-state**: Confirms optimizer effect

**What to look for**:
- `alpha_layerwise.png`: Which layers are most heavy-tailed?
- `early_vs_steady.png`: Does Î± evolve during training?
- Are vector params (embeddings, norms) different from matrices?

## ðŸ§ª Advanced Usage

### Custom Gradient Distributions (Exp 1.1)

Test different upstream gradient distributions:

```yaml
synthetic:
  grad_dist: "cauchy"  # Heavy-tailed (Î± = 1)
  # Options: normal, cauchy, laplace, uniform
```

### Modify Tail Estimators

Change which estimators to use:

```yaml
measurement:
  estimators: ['hill', 'pickands']  # Faster, no ML
  k_ratios: [0.1]  # Single k-ratio
```

### Adjust Measurement Frequency

For faster experiments:

```yaml
measurement:
  alpha_interval: 500  # Measure less frequently
  continuous_interval: 500
```

## ðŸ› Troubleshooting

### Import Errors

**Problem**: `ModuleNotFoundError: No module named 'levy'`

**Solution**:
```bash
pip install levy
# Or remove 'ml' from estimators in config
```

### Wandb Authentication

**Problem**: `wandb.errors.UsageError: api_key not configured`

**Solution**:
```bash
wandb login
# Or set use_wandb: false in config
```

### CUDA Out of Memory

**Problem**: `RuntimeError: CUDA out of memory`

**Solution**:
- Reduce batch size in config
- Use smaller model widths
- Run on CPU (slower but works)

### NaN in Alpha Estimates

**Problem**: `Î± = nan` in logs

**Possible Causes**:
- Too few gradient samples (increase batch size)
- Gradients are all zero (check model/data)
- Extreme gradient magnitudes (check for numerical instability)

**Debug**:
```python
# Add to experiment code
print(f"Grad norm: {model.W_in.weight.grad.norm()}")
print(f"Grad min/max: {model.W_in.weight.grad.min()}/{model.W_in.weight.grad.max()}")
```

## ðŸ“š Theoretical Background

### Î±-Stable Distributions

Î±-stable distributions are characterized by:
- **Î± âˆˆ (0, 2]**: Stability parameter (tail index)
  - Î± = 2: Gaussian
  - Î± = 1: Cauchy
  - Î± < 2: Heavy-tailed (infinite variance)

- **Key Property**: Stable under addition
  - Sum of Î±-stable variables is Î±-stable
  - Generalized Central Limit Theorem

### Batch Size Scaling

**Standard (Gaussian) Theory**:
```
Gradient = True_gradient + Noise/âˆšB
â†’ Optimal LR âˆ âˆšB
```

**Heavy-Tailed Theory**:
```
Gradient = True_gradient + Noise/B^(1/Î±)
â†’ Optimal LR âˆ B^(1/Î± - 1)
```

**Example** (Î± = 1.5):
- Standard: LR âˆ B^0.5
- Reality: LR âˆ B^(-0.33)
- **Implication**: Large batches need LOWER learning rates!

### Steady-State Scaling

According to Kosson et al. (2023):
```
||W||_steady âˆ âˆš(Î·/Î») Â· d^0.75
```

Where:
- Î·: Learning rate
- Î»: Weight decay
- d: Model width

**Our extension**:
```
||W||_steady âˆ âˆš(Î·/Î») Â· d^0.75 Â· B^(1/2Î± - 1/2)
```

## ðŸ“– References

1. **Kosson et al. (2025)** - "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning"
2. **Yang et al. (2022)** - "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer" (Î¼P)
3. **Nolan, J. P. (2020)** - "Univariate Stable Distributions: Models for Heavy Tailed Data"
4. **Hill, B. M. (1975)** - "A Simple General Approach to Inference About the Tail of a Distribution"

## ðŸ¤ Contributing

This is a research project. For questions or discussions about the implementation:

1. Check `IMPLEMENTATION_DECISIONS.md` for detailed design rationale
2. Review experiment configs and adjust parameters
3. Examine output logs and plots for insights

## ðŸ“ Citation

If you use this code or findings in your research, please cite:

```bibtex
@software{heavy_tail_scaling_2025,
  title = {Heavy-Tailed Gradient Scaling Laws Research},
  year = {2025},
  note = {Phase 1: Establishing the Heavy-Tail Phenomenon}
}
```

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸ”® Future Work (Phases 2-4)

**Phase 2**: Batch Size Scaling
- Test Î·_opt vs B relationship
- Validate B^(1/Î± - 1) scaling

**Phase 3**: Joint Width-Batch-Decay Scaling
- Full grid search over (d, B, Î·, Î»)
- Unified scaling law derivation

**Phase 4**: The Muon Connection
- Compare tail behavior: SGD vs AdamW vs Muon
- Test "tail taming" hypothesis

---

**Status**: Phase 1 Implementation Complete âœ…

For detailed implementation decisions and expert review notes, see `IMPLEMENTATION_DECISIONS.md`.
