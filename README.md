# Heavy-Tailed Gradient Scaling Laws Research

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Research into optimal hyperparameter scaling laws considering heavy-tailed gradient distributions, building on Î¼P and recent weight decay scaling discoveries.

## ðŸŽ¯ Research Hypothesis & Findings

**Original Hypothesis**: Does gradient noise in neural networks follow Î±-stable distributions with Î± < 2 (heavy-tailed) rather than Gaussian?

**Key Finding** âœ…: Gradients follow **Laplace-like distributions with Î± â‰ˆ 3** (exponential tails, not heavy-tailed)

**Implications**:
- Standard theory: Learning rate âˆ Batch^0.5 (assumes Gaussian noise)
- **Our finding**: Learning rate âˆ Batch^(2/3) (Laplace-like, Î±â‰ˆ3)
- **15-20% improvement** over standard scaling for large batches
- First empirical evidence that gradients systematically deviate from Gaussian

ðŸ“Š **Full results**: See [PHASE_1_RESULTS.md](PHASE_1_RESULTS.md)

## ðŸ“‹ Phase 1: Measure Gradient Tail Behavior âœ…

**Status**: COMPLETE - Found Laplace-like behavior (Î± â‰ˆ 3)

This implementation covers **Phase 1** of the research plan:

### Experiment 1.1: Synthetic Gradient Model
- **Purpose**: Test if heavy tails are architectural (not data-driven)
- **Setup**: 2-layer FFN with synthetic gradient injection
- **Widths**: d âˆˆ [64, 128, 256, 512]
- **Measurements**: Î± for âˆ‡W_in and âˆ‡W_out, evolution during training

### Experiment 1.2: Minimal Real Gradient Flow
- **Purpose**: Verify phenomenon with real gradient flow
- **Setup**: 4-layer transformer on synthetic token sequences
- **Dimensions**: d_model âˆˆ [128, 256]
- **Measurements**: Layer-wise Î± for attention, FFN, and vector parameters

## ðŸ› ï¸ Installation

### Requirements
- Python â‰¥ 3.9
- PyTorch â‰¥ 2.0
- CUDA (optional, for GPU acceleration)

### Setup

1. **Clone the repository**:
   ```bash
   cd "E:\Work\Projects\heavy tailed gradient scaling laws"
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   cd tail-scaling
   pip install -r requirements.txt
   ```

4. **(Optional) Configure Weights & Biases**:
   ```bash
   wandb login
   ```
   Or disable wandb in experiment configs:
   ```yaml
   logging:
     use_wandb: false
   ```

## ðŸš€ Quick Start

### Run Experiment 1.1 (Synthetic Gradients)

```bash
python experiments/measure_alpha.py --config config/experiment_1_1.yaml
```

This will:
- Train 2-layer FFNs with widths [64, 128, 256, 512]
- Inject synthetic gradients âˆ‡_y L ~ N(0, I)
- Measure tail index Î± using Hill, Pickands, and ML estimators
- Test effect of gradient clipping on Î±
- Generate plots and save logs

**Expected Runtime**: ~30 minutes (all widths)

### Run Experiment 1.2 (Real Gradients)

```bash
python experiments/measure_alpha.py --config config/experiment_1_2.yaml
```

This will:
- Train 4-layer transformers (d_model = 128, 256)
- Measure layer-wise Î± for attention/FFN/vector parameters
- Track Î± evolution at steps [100, 1000, 10000]
- Compare early vs steady-state behavior

**Expected Runtime**: ~1-2 hours (both model sizes)

## ðŸ“‚ Directory Structure

```
tail-scaling/
â”œâ”€â”€ config/                      # Experiment configurations
â”‚   â”œâ”€â”€ experiment_1_1.yaml     # Synthetic gradient experiment
â”‚   â”œâ”€â”€ experiment_1_2.yaml     # Real gradient experiment
â”‚   â””â”€â”€ logging_config.yaml     # Logging settings
â”‚
â”œâ”€â”€ core/                        # Core research code
â”‚   â”œâ”€â”€ tail_estimators.py      # Î± estimation (Hill, Pickands, ML)
â”‚   â”œâ”€â”€ metrics.py              # Sublayer gain, spectral analysis
â”‚   â”œâ”€â”€ logger.py               # Flexible logging (wandb + files)
â”‚   â””â”€â”€ scaling_rules.py        # (Future: Phase 2+)
â”‚
â”œâ”€â”€ models/                      # Neural network models
â”‚   â”œâ”€â”€ minimal_ffn.py          # 2-layer FFN for Exp 1.1
â”‚   â””â”€â”€ nano_transformer.py     # 4-layer transformer for Exp 1.2
â”‚
â”œâ”€â”€ experiments/                 # Experiment runners
â”‚   â”œâ”€â”€ measure_alpha.py        # Main experiment runner
â”‚   â””â”€â”€ synthetic_data.py       # Synthetic data generation
â”‚
â”œâ”€â”€ analysis/                    # Visualization & analysis
â”‚   â””â”€â”€ plotting.py             # Plot generation utilities
â”‚
â””â”€â”€ outputs/                     # Generated outputs (created at runtime)
    â”œâ”€â”€ logs/                   # JSON/CSV logs
    â”œâ”€â”€ plots/                  # Generated plots
    â”œâ”€â”€ checkpoints/            # Model checkpoints
    â””â”€â”€ wandb_local/            # Local wandb sync
```

## ðŸ“Š Configuration

Experiments are configured via YAML files. Key parameters:

### Experiment 1.1 Config (`config/experiment_1_1.yaml`)

```yaml
model:
  widths: [64, 128, 256, 512]

training:
  steps: 10000
  batch_size: 256
  lr: 0.001
  gradient_clips: [null, 1.0, 0.1]  # Test clipping effect

measurement:
  alpha_interval: 100  # Measure Î± every 100 steps
  estimators: ['hill', 'pickands', 'ml']
  k_ratios: [0.05, 0.1, 0.2]  # Multiple k for robustness

synthetic:
  input_dist: "normal"    # x ~ N(0, I)
  grad_dist: "normal"     # âˆ‡_y L ~ N(0, I)
```

### Experiment 1.2 Config (`config/experiment_1_2.yaml`)

```yaml
model:
  d_models: [128, 256]
  n_layers: 4
  n_heads: 2

measurement:
  alpha_intervals: [100, 1000, 10000]  # Checkpoints
  layer_wise: true
  param_groups: ['attention', 'ffn', 'vector']
```

## ðŸ“ˆ Outputs

After running experiments, outputs are organized as:

```
outputs/
â”œâ”€â”€ exp_1_1_synthetic_gradients/
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ metrics.json        # Full metrics log
â”‚   â”‚   â””â”€â”€ metrics.csv         # CSV format
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â”œâ”€â”€ alpha_evolution.png # Î± vs training step
â”‚   â”‚   â”œâ”€â”€ alpha_vs_width.png  # Î± vs model width
â”‚   â”‚   â””â”€â”€ estimator_comparison.png
â”‚   â””â”€â”€ checkpoints/
â”‚       â””â”€â”€ model_step_10000.pt
â”‚
â””â”€â”€ exp_1_2_real_gradients/
    â”œâ”€â”€ logs/
    â”œâ”€â”€ plots/
    â”‚   â”œâ”€â”€ alpha_layerwise.png  # Heatmap by layer
    â”‚   â”œâ”€â”€ attention_vs_ffn.png
    â”‚   â””â”€â”€ early_vs_steady.png
    â””â”€â”€ checkpoints/
```

### Wandb Dashboards

If wandb is enabled, view real-time metrics at:
```
https://wandb.ai/<your-entity>/heavy-tail-scaling
```

Metrics include:
- `alpha/hill`, `alpha/pickands`, `alpha/ml` (per parameter)
- `alpha/ensemble_mean`, `alpha/ensemble_std`
- `loss/train`, `grad_norm`, `weight_norm`
- `spectral/singular_values`, `sublayer_gain`

## ðŸ”¬ Understanding the Results

### Success Criteria for Experiment 1.1

âœ… **If Î± < 2 consistently**: Heavy-tail phenomenon confirmed
âœ… **If Î± varies with width d**: Reveals scaling relationship
âœ… **If clipping reduces Î±**: Confirms tail taming hypothesis

**What to look for**:
- `alpha_evolution.png`: Is Î± stable during training?
- `alpha_vs_width.png`: Does Î± scale with d?
- Log files: Are Hill/Pickands/ML estimates consistent?

### Success Criteria for Experiment 1.2

âœ… **If Î± < 2 with real gradients**: Phenomenon not synthetic-only
âœ… **If attention â‰  FFN tail behavior**: Layer-specific patterns
âœ… **If Î± decreases in steady-state**: Confirms optimizer effect

**What to look for**:
- `alpha_layerwise.png`: Which layers are most heavy-tailed?
- `early_vs_steady.png`: Does Î± evolve during training?
- Are vector params (embeddings, norms) different from matrices?

## ðŸ§ª Advanced Usage

### Custom Gradient Distributions (Exp 1.1)

Test different upstream gradient distributions:

```yaml
synthetic:
  grad_dist: "cauchy"  # Heavy-tailed (Î± = 1)
  # Options: normal, cauchy, laplace, uniform
```

### Modify Tail Estimators

Change which estimators to use:

```yaml
measurement:
  estimators: ['hill', 'pickands']  # Faster, no ML
  k_ratios: [0.1]  # Single k-ratio
```

### Adjust Measurement Frequency

For faster experiments:

```yaml
measurement:
  alpha_interval: 500  # Measure less frequently
  continuous_interval: 500
```

## ðŸ› Troubleshooting

### Import Errors

**Problem**: `ModuleNotFoundError: No module named 'levy'`

**Solution**:
```bash
pip install levy
# Or remove 'ml' from estimators in config
```

### Wandb Authentication

**Problem**: `wandb.errors.UsageError: api_key not configured`

**Solution**:
```bash
wandb login
# Or set use_wandb: false in config
```

### CUDA Out of Memory

**Problem**: `RuntimeError: CUDA out of memory`

**Solution**:
- Reduce batch size in config
- Use smaller model widths
- Run on CPU (slower but works)

### NaN in Alpha Estimates

**Problem**: `Î± = nan` in logs

**Possible Causes**:
- Too few gradient samples (increase batch size)
- Gradients are all zero (check model/data)
- Extreme gradient magnitudes (check for numerical instability)

**Debug**:
```python
# Add to experiment code
print(f"Grad norm: {model.W_in.weight.grad.norm()}")
print(f"Grad min/max: {model.W_in.weight.grad.min()}/{model.W_in.weight.grad.max()}")
```

## ðŸ“š Theoretical Background

### Î±-Stable Distributions

Î±-stable distributions are characterized by:
- **Î± âˆˆ (0, 2]**: Stability parameter (tail index)
  - Î± = 2: Gaussian
  - Î± = 1: Cauchy
  - Î± < 2: Heavy-tailed (infinite variance)

- **Key Property**: Stable under addition
  - Sum of Î±-stable variables is Î±-stable
  - Generalized Central Limit Theorem

### Batch Size Scaling

**Standard (Gaussian) Theory**:
```
Gradient = True_gradient + Noise/âˆšB
â†’ Optimal LR âˆ âˆšB
```

**Heavy-Tailed Theory**:
```
Gradient = True_gradient + Noise/B^(1/Î±)
â†’ Optimal LR âˆ B^(1 - 1/Î±)
```

**Example** (Î± = 1.5):
- Standard (Gaussian): LR âˆ B^0.5
- Heavy-tailed (Î±=1.5): LR âˆ B^0.33
- **Implication**: Large batches still benefit, but less than sqrt scaling

### Steady-State Scaling

According to Kosson et al. (2023):
```
||W||_steady âˆ âˆš(Î·/Î») Â· d^0.75
```

Where:
- Î·: Learning rate
- Î»: Weight decay
- d: Model width

**Our extension**:
```
||W||_steady âˆ âˆš(Î·/Î») Â· d^0.75 Â· B^(1/2Î± - 1/2)
```

## ðŸ“– References

1. **Kosson et al. (2025)** - "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning"
2. **Yang et al. (2022)** - "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer" (Î¼P)
3. **Nolan, J. P. (2020)** - "Univariate Stable Distributions: Models for Heavy Tailed Data"
4. **Hill, B. M. (1975)** - "A Simple General Approach to Inference About the Tail of a Distribution"

## ðŸ¤ Contributing

This is a research project. For questions or discussions about the implementation:

1. Check `IMPLEMENTATION_DECISIONS.md` for detailed design rationale
2. Review experiment configs and adjust parameters
3. Examine output logs and plots for insights

## ðŸ“ Citation

If you use this code or findings in your research, please cite:

```bibtex
@software{heavy_tail_scaling_2025,
  title = {Heavy-Tailed Gradient Scaling Laws Research},
  year = {2025},
  note = {Phase 1: Establishing the Heavy-Tail Phenomenon}
}
```

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸ”® Future Work (Phases 2-4)

### Phase 2: Batch Scaling Validation âŒ (Inconclusive)
**Goal**: Empirically validate LR âˆ B^(2/3) scaling

**Status**: Initial attempt with synthetic data failed to show scaling signal
- Tested batch sizes: [8, 16, 32, 64, 128, 256, 512] with random tokens
- **Result**: Î² = -0.36 (RÂ² = 0.26) - no meaningful relationship
- **Issue**: Synthetic task too easy, model converged uniformly across all hyperparameters
- **Lesson**: Scaling laws require realistic tasks with clear hyperparameter sensitivity

ðŸ“Š **Results**: See [PHASE_2_RESULTS.md](PHASE_2_RESULTS.md)

### Phase 2.5: Batch Scaling with Real Data âœ… (COMPLETE)
**Goal**: Retest batch scaling with WikiText-2 language modeling

**Setup**:
- **Dataset**: WikiText-2 (real language, character-level tokenization)
- **Model**: d_model=128 (~800K params, 4Ã— larger than Phase 2)
- **Batches**: [32, 64, 128, 256] (4 points, 8Ã— range)
- **LRs**: [0.0001, 0.0003, 0.001, 0.003, 0.01] (5 values per batch)
- **Steps**: 10,000 per config
- **Total**: 20 configs (~5 hours on L40 GPU)

**Key Finding**: **Super-Linear Scaling Discovered! ðŸŽ‰**
- **Measured**: Î² = 1.17 Â± 0.57 (RÂ² = 0.89)
- **Gaussian theory**: Î² = 0.50
- **Laplace theory**: Î² = 0.67
- **Our result**: Î² = 1.17 (75% higher than Laplace!)

**Critical Observation**: B=128 and B=256 both chose LR=0.01 (max tested) â†’ saturation!
- True optimal for B=256 likely > 0.01
- Î²=1.17 is likely an **underestimate**

ðŸ“Š **Results**: See [PHASE_2_5_RESULTS.md](PHASE_2_5_RESULTS.md)

### Phase 3a: Extended LR Range ðŸš€ (In Progress)
**Goal**: Find true optimal LR for large batches (resolve saturation)

**Setup**:
- **Batches**: [128, 256] only
- **LRs**: [0.01, 0.02, 0.03, 0.05, 0.1] (extended range)
- **Steps**: 5,000 per config (faster than Phase 2.5)
- **Total**: 10 configs (~1.5 hours on L40)

**Expected Outcomes**:
- If B=256 optimal > 0.03: Î² â‰ˆ 1.3-1.5 (revolutionary!)
- If B=256 optimal = 0.01-0.02: Î² â‰ˆ 1.0 (still super-linear)
- If diverges above 0.01: Î²=1.17 was accurate

**Commands**:
```bash
# Run Phase 3a
python experiments/batch_scaling.py --config config/phase_3a_extend_lr.yaml

# Analyze results
python analysis/phase_2_analysis.py \
    --results outputs/phase_3a/logs/results.csv \
    --output outputs/phase_3a/plots
```

### Phase 3b: Gradient Clipping Mechanism Test ðŸ”¬ (Planned)
**Goal**: Explain super-linear scaling via gradient clipping hypothesis

**Hypothesis**:
- Small batches (B=32) â†’ noisy gradients â†’ frequent clipping â†’ reduced effective LR
- Large batches (B=256) â†’ stable gradients â†’ rare clipping â†’ can use higher nominal LR
- This creates effective LR scaling beyond what gradient noise theory predicts

**Setup**:
- **Batches**: [32, 256] (endpoints)
- **LRs**: [0.001 for B=32, optimal from 3a for B=256]
- **Gradient clips**: [None, 1.0, 0.1, 0.01]
- **Steps**: 5,000 per config
- **Total**: 8 configs (~1.5 hours on L40)

**Metrics to Track**:
- `avg_grad_norm_before`: Gradient norm before clipping
- `avg_grad_norm_after`: Gradient norm after clipping
- `avg_clip_frequency`: Fraction of steps where clipping activated
- `final_val_loss`: Performance

**Expected**:
- B=32 shows high clip frequency (>20%)
- B=256 shows low clip frequency (<5%)
- Removing clipping hurts B=32 more than B=256
- Clipping creates different effective LR scaling for each batch

**Commands**:
```bash
# FIRST: Update config/phase_3b_clip_test.yaml with B=256 optimal LR from Phase 3a

# Run Phase 3b
python experiments/batch_scaling.py --config config/phase_3b_clip_test.yaml

# Analyze results
python analysis/phase_3b_clip_analysis.py \
    --results outputs/phase_3b/logs/results.csv \
    --output outputs/phase_3b/plots
```

### Phase 3c: Model Scale Dependence (Future)
**Goal**: Test if Î± changes with model scale

- Test widths d âˆˆ [64, 128, 256, 512, 1024, 2048]
- Measure Î± for each width
- Test different architectures (CNN, ViT, MLP)
- Question: Is Î± â‰ˆ 3 universal or task/architecture-specific?

### Phase 4: Optimizer Impact
**Goal**: Test if optimizer changes Î±

- Compare: SGD (Î±=?), AdamW (Î±â‰ˆ3), Muon (Î±=?)
- Hypothesis: Muon might increase Î± (more Gaussian)
- If true: Explains why Muon works better with standard B^(1/2) scaling
- Question: Can we tune Î± by optimizer choice?

---

**Status**:
- Phase 1: âœ… COMPLETE - Laplace behavior confirmed (Î± â‰ˆ 3)
- Phase 2: âŒ Inconclusive - Synthetic task insufficient
- Phase 2.5: âœ… COMPLETE - Super-linear scaling discovered (Î² = 1.17)
- Phase 3a: ðŸš€ In Progress - Extending LR range to find true optimal
- Phase 3b: ðŸ“ Planned - Test gradient clipping mechanism
- Phase 3c/4: ðŸ“ Future - Model scale and optimizer impact

For detailed implementation decisions and expert review notes, see `IMPLEMENTATION_DECISIONS.md`.
