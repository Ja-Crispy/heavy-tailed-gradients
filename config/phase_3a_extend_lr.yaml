# Phase 3a: Extended LR Range for Large Batches
# Goal: Find true optimal LR for B=128 and B=256 (avoid saturation at 0.01)
# Phase 2.5 showed both batches chose LR=0.01 (max tested), suggesting true optimal > 0.01

experiment:
  name: "phase_3a_extend_lr"
  seed: 42
  output_dir: "outputs/phase_3a"
  description: "Extended LR sweep for B=128 and B=256 to find true optimal beyond 0.01"

model:
  type: "nano_transformer"
  d_model: 128  # Same as Phase 2.5
  n_layers: 4
  n_heads: 4
  vocab_size: 128  # Character-level
  seq_length: 256
  dropout: 0.0

training:
  steps: 5000  # Faster than Phase 2.5 (10k) - sufficient to measure optimal
  optimizer: "adamw"
  betas: [0.9, 0.999]
  weight_decay: 0.01
  warmup_steps: 100  # 2% of training
  grad_clip: 1.0  # Same as Phase 2.5

  # Convergence criteria
  eval_interval: 100
  convergence_window: 50  # Average final 50 steps (shorter window for 5k steps)
  convergence_threshold: 0.01  # Loss std < 0.01 for convergence

batch_sweep:
  # Only test large batches that hit LR saturation in Phase 2.5
  batch_sizes: [128, 256]

  # Extended LR range: Start at Phase 2.5 max (0.01), go up to 10× higher
  # Hypothesis: B=256 optimal is ~0.02-0.05 based on β=1.17 trend
  lr_candidates: [0.01, 0.02, 0.03, 0.05, 0.1]

  # Total experiments: 2 batch sizes × 5 LR values = 10 configs
  # Expected runtime:
  #   - L40 GPU: ~1-1.5 hours (10 × 5000 steps × ~5-10 min)
  #   - 3050 GPU: ~3-4 hours (10 × 5000 steps × ~15-20 min)

dataset:
  name: "wikitext2"
  split: "train"
  tokenizer: "char"  # Character-level tokenization
  data_dir: "data/wikitext-2"
  max_sequences: null  # Use all available sequences

logging:
  use_wandb: true
  wandb_project: "laplace-scaling-phase3a"
  wandb_entity: null  # Use default
  log_interval: 50
  save_checkpoints: false

  # Save results to CSV for analysis
  save_results_csv: true
  results_file: "results.csv"

# Analysis settings (use same script as Phase 2.5)
analysis:
  fit_method: "log_log_regression"

  # Expected outcomes:
  # - If B=256 optimal > 0.03: β likely > 1.3 (super-linear confirmed!)
  # - If B=256 optimal = 0.01-0.02: β ≈ 1.0 (still super-linear)
  # - If B=256 diverges above 0.01: β = 1.17 was accurate (edge of stability)

  # Will update power-law fit with extended data points
  laplace_beta: 0.667
  gaussian_beta: 0.500
