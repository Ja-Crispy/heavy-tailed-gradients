# Phase 2.5: Batch Scaling Validation with Real Data
# Goal: Retest LR ∝ Batch^(2/3) scaling with WikiText-2 language modeling
# Previous Phase 2 failed due to synthetic task being too easy

experiment:
  name: "phase_2_5_wikitext"
  seed: 42
  output_dir: "outputs/phase_2_5"
  description: "Batch scaling with WikiText-2: Test LR_opt vs Batch with real language data"

model:
  type: "nano_transformer"
  d_model: 128  # Increased from 64 (4× larger, ~800K params)
  n_layers: 4
  n_heads: 4
  vocab_size: 128  # ASCII character-level tokenization
  seq_length: 256
  dropout: 0.0

training:
  steps: 10000  # Phase 1 standard (increased from Phase 2's 5000)
  optimizer: "adamw"
  betas: [0.9, 0.999]
  weight_decay: 0.01
  warmup_steps: 200  # 2% of training
  grad_clip: 1.0

  # Convergence criteria
  eval_interval: 100
  convergence_window: 100  # Average final 100 steps
  convergence_threshold: 0.01  # Loss std < 0.01 for convergence

batch_sweep:
  # Test 4 batch sizes covering 8× range (32 → 256)
  # Avoiding B=512 which was extremely slow in Phase 2
  batch_sizes: [32, 64, 128, 256]

  # Test 5 LR values per batch size (finer grid than Phase 2)
  lr_candidates: [0.0001, 0.0003, 0.001, 0.003, 0.01]

  # Total experiments: 4 batch sizes × 5 LR values = 20 configs
  # Expected runtime:
  #   - Colab T4: ~3-5 hours (20 × 10000 steps × ~1 min)
  #   - Local 3050: ~8-12 hours (20 × 10000 steps × ~3 min)

dataset:
  name: "wikitext2"
  split: "train"  # Use 'val' for validation
  tokenizer: "char"  # Character-level tokenization
  data_dir: "data/wikitext-2"
  max_sequences: null  # Use all available sequences
  # Note: WikiTextDataset will download automatically if not present

logging:
  use_wandb: true
  wandb_project: "laplace-scaling-phase2-5"  # New project for Phase 2.5
  wandb_entity: null  # Use default
  log_interval: 50
  save_checkpoints: false  # Don't save model checkpoints

  # Save results to CSV for analysis
  save_results_csv: true
  results_file: "results.csv"

# Analysis settings (used by phase_2_analysis.py)
analysis:
  # Power law fit: LR_opt = C · Batch^β
  fit_method: "log_log_regression"

  # Expected scaling exponents
  laplace_beta: 0.667  # α=3: β = 1 - 1/3 = 2/3
  gaussian_beta: 0.500  # α=∞: β = 1/2

  # Success criteria
  laplace_range: [0.60, 0.74]  # If β in this range → Laplace confirmed
  gaussian_range: [0.45, 0.55]  # If β in this range → Gaussian behavior

  # Transfer test settings
  baseline_batch: 128  # Changed from 256 (more centered in our range)
  baseline_lr: 0.001
